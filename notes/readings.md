# Readings

### Closely related to core idea
- [Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks](https://openreview.net/forum?id=7uVcpu-gMD)
    - Learns binary weight masks for a visual task and two language tasks.
- [Break It Down: Evidence for Structural Compositionality in Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2023/hash/85069585133c4c168c865e65d72e9775-Abstract-Conference.html)
    - Learns weight masks for SCAN, but do not try to isolate functions.
- [Winning the Lottery with Continuous Sparsification](https://proceedings.neurips.cc/paper/2020/hash/83004190b1793d7aa15f8d0d49a13eba-Abstract.html)
    - Introduces the binary weight mask technique.
- [Measuring Compositionality in Representation Learning](https://openreview.net/forum?id=HJz05o0qK7)
    - Introduces the Tree Reconstruction Error that measures
      compositionality in representations.

### Mechanistic Interpretability
- [A Primer on the Inner Workings of Transformer-based Language Models](https://arxiv.org/abs/2405.00208)
- [A Toy Model of Universality: Reverse Engineering how Networks Learn Group Operations](https://proceedings.mlr.press/v202/chughtai23a.html)
- [Convergent Learning: Do different neural networks learn the same representations?](http://proceedings.mlr.press/v44/li15convergent.html)
- [Towards Automated Circuit Discovery for Mechanistic Interpretability](https://proceedings.neurips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html)
- [Circuit Component Reuse Across Tasks in Transformer Language Models](https://openreview.net/forum?id=fpoAYV6Wsk)

### Compositionality
- [Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks (SCAN paper)](https://proceedings.mlr.press/v80/lake18a.html)
- [Compositionality Decomposed: How do Neural Networks Generalise? (PCFGS paper)](https://www.jair.org/index.php/jair/article/view/11674)
- [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://arxiv.org/abs/2401.06102)
- [Faith and Fate: Limits of Transformers on Compositionality](https://proceedings.neurips.cc/paper_files/paper/2023/hash/deb3c28192f979302c157cb653c15e90-Abstract-Conference.html)
- [Compositional generalization through abstract representations in human and artificial neural networks](https://proceedings.neurips.cc/paper_files/paper/2022/hash/d0241a0fb1fc9be477bdfde5e0da276a-Abstract-Conference.html)
- [The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers](https://aclanthology.org/2021.emnlp-main.49/)




