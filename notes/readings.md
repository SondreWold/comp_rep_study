# Readings

### Core
- [Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks](https://openreview.net/forum?id=7uVcpu-gMD)
    - Learns binary weight masks for a visual task and two language tasks.
- [Break It Down: Evidence for Structural Compositionality in Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2023/hash/85069585133c4c168c865e65d72e9775-Abstract-Conference.html)
    - Learns weight masks for SCAN, but do not try to isolate functions.
- [Winning the Lottery with Continuous Sparsification](https://proceedings.neurips.cc/paper/2020/hash/83004190b1793d7aa15f8d0d49a13eba-Abstract.html)
    - Introduces the binary weight mask technique.

### Mechanistic Interpretability
- [A Toy Model of Universality: Reverse Engineering how Networks Learn Group Operations](https://proceedings.mlr.press/v202/chughtai23a.html)
- [Convergent Learning: Do different neural networks learn the same representations?](http://proceedings.mlr.press/v44/li15convergent.html)
- [Towards Automated Circuit Discovery for Mechanistic Interpretability](https://proceedings.neurips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html)

### Compositionality
- [Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks (SCAN paper)](https://proceedings.mlr.press/v80/lake18a.html)
- [Compositionality Decomposed: How do Neural Networks Generalise? (PCFGS paper)](https://www.jair.org/index.php/jair/article/view/11674)


